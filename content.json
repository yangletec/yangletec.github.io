{"meta":{"title":"AI码师","subtitle":"授人以鱼不如授人以渔","description":"拥有五年一线大厂开发经验，擅长分布式、微服务、性能调优、源码分析、并发编程、面试经验分享等众多技能,关注公众号“AI码师”领取2021最新面试资料一份","author":"AI码师","url":"https://yangletec.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-02-28T14:42:18.912Z","updated":"2021-02-28T14:42:18.912Z","comments":false,"path":"/404.html","permalink":"https://yangletec.github.io/404.html","excerpt":"","text":""},{"title":"关于","date":"2021-02-28T14:42:18.912Z","updated":"2021-02-28T14:42:18.912Z","comments":false,"path":"about/index.html","permalink":"https://yangletec.github.io/about/index.html","excerpt":"","text":"自语 大家好，我是一枚会前端的后端开发工程师 👨🏻‍💻，目前主攻 Java 后端开发. 我希望用代码改变世界,我也在改变世界,自豪脸 😏 目前正走在成为一个厉害(Zhuang Bi)程序猿的路上. 前途很艰辛, 但是我会坚持! 生活不止有撸码和工作, 也有诗和远方. 在快乐中生活, 在快乐中工作, 爱家人, 爱自己 子曰：生而知之者，上也；学而知之者，次也；困而学之，又其次也；困而不学，民斯为下矣。人生得意须尽欢，莫使金樽空对月。少小须勤学，文章可立身。满朝朱紫贵，尽是读书人。 技能 语言：Java, JavaScript, HTML, CSS 数据库：MySQL, Redis, Oracle 开发框架：Spring, Spring Boot, Spring Cloud, MyBatis, Guice, Shiro, React, JQuery，Bootstrap 中间件：Kafka, Zookeeper 构建工具： Maven, Tomcat 环境： Mac, Ubuntu, Linux, Git, Intellij IDEA 学习 有道是学无止境，Java 后端开发的路很难走，但是既然走了这条路，那就得一直走下去。 Java 相关的技术栈很多，学起来很不容易，最近两年的行情也是优胜劣汰，希望自己能够快速成长起来 夯实基础，学习源码 今年想要学习一下 python 博客 期望：平时生活和工作一种记录, 希望之后回过头再来看自己的博客也会有所收获或感想 技术、电影与生活 对生活和学习的一个记录 希望自己越来越强大，越来越开心 免责声明 本站所有文章为了记录工作、学习中遇到的问题，可能由于本人技术有限，有些不正确的地方，仅供参考 本站文章引用或转载写明来源，感谢原作者的辛苦写作，如果有异议或侵权，及时联系我处理，谢谢！ 如他人引用本站中的文章或内容，请注明出处。但其文章或内容已不是本人原本的意思，请各位注意辨别！ 本站所有文章仅代表个人当时意见和想法 欢迎指出有问题的地方，我会尽快修正，谢谢！ 内容转载请保留署名以及原文连接，谢谢? 本网站所有作品采用 知识共享署名-相同方式共享 4.0 国际许可协议 进行许可。 关注我关注公众号“AI 码师”领取 2021 最新面试资料一份"},{"title":"书单","date":"2021-02-28T14:42:18.912Z","updated":"2021-02-28T14:42:18.912Z","comments":false,"path":"books/index.html","permalink":"https://yangletec.github.io/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2021-02-28T14:42:18.912Z","updated":"2021-02-28T14:42:18.912Z","comments":false,"path":"categories/index.html","permalink":"https://yangletec.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-02-28T14:42:18.912Z","updated":"2021-02-28T14:42:18.912Z","comments":true,"path":"links/index.html","permalink":"https://yangletec.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-02-28T14:42:18.912Z","updated":"2021-02-28T14:42:18.912Z","comments":false,"path":"repository/index.html","permalink":"https://yangletec.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-02-28T14:42:18.912Z","updated":"2021-02-28T14:42:18.912Z","comments":false,"path":"tags/index.html","permalink":"https://yangletec.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hello World","slug":"hello-world","date":"2021-02-28T14:42:18.912Z","updated":"2021-02-28T14:42:18.912Z","comments":true,"path":"post/4a17b156.html","link":"","permalink":"https://yangletec.github.io/post/4a17b156.html","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"性能优化｜解读面试题，彻底搞懂类加载和初始化顺序","slug":"yuque/性能优化｜解读面试题，彻底搞懂类加载和初始化顺序","date":"2021-02-28T14:23:26.000Z","updated":"2021-02-28T14:42:47.594Z","comments":true,"path":"post/27c9d2d5.html","link":"","permalink":"https://yangletec.github.io/post/27c9d2d5.html","excerpt":"","text":"解读面试题，彻底搞懂类加载和初始化顺序在高级面试过程中，始终逃不过面试官的追问三连： 你知道 jvm 是怎么加载类的么？ 类的初始化顺序你有了解么？ 我出一个面试题，你能答出来么？ 三连问下来，恐怕自己已经被劝退了，有的同学肯定会学过 jvm 是如何加载类的，但是被面试官疑问，就一脸懵了，主要原因是没有掌握到精髓，没有知道其中的原理，光靠死记硬背是不行的，面试官都看在眼里的。本文将带着大家一起分析面试题，来梳理下其中主要的知识点，相信大家在看完之后，也一定会有收获的，可以彻底告别面试官的连连追问了。 面试题 1现在我们进入正题：面试官直接抛过来第一道面试题，看看大家能猜出结果不 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package org.apache.dubbo.demo.provider;public class JVMClass extends BaseCodeBlock &#123; &#123; System.out.println(&quot;子类的普通代码块&quot;); &#125; public JVMClass() &#123; System.out.println(&quot;子类的构造方法&quot;); &#125; @Override public void msg() &#123; System.out.println(&quot;子类的普通方法&quot;); &#125; public static void msg2() &#123; System.out.println(&quot;子类的静态方法&quot;); &#125; static &#123; System.out.println(&quot;子类的静态代码块&quot;); &#125; public static void main(String[] args) &#123; BaseCodeBlock bcb &#x3D; new JVMClass(); bcb.msg(); &#125; Other o &#x3D; new Other();&#125;class BaseCodeBlock &#123; public BaseCodeBlock() &#123; System.out.println(&quot;父类的构造方法&quot;); &#125; public void msg() &#123; System.out.println(&quot;父类的普通方法&quot;); &#125; public static void msg2() &#123; System.out.println(&quot;父类的静态方法&quot;); &#125; static &#123; System.out.println(&quot;父类的静态代码块&quot;); &#125; Other2 o2 &#x3D; new Other2(); &#123; System.out.println(&quot;父类的普通代码块&quot;); &#125;&#125;class Other &#123; Other() &#123; System.out.println(&quot;初始化子类的属性值&quot;); &#125;&#125;class Other2 &#123; Other2() &#123; System.out.println(&quot;初始化父类的属性值&quot;); &#125;&#125; 大家可以把上面的代码拷贝到编辑器上面，执行下看看和自己预期的结果是否一致，这段代码基本上可以展示出了类加载和初始化顺序，给大家看下结果可以看出如果有继承父类的话，会优先去初始化父类。遵循这样一个顺序 父类的静态代码块-&gt;子类的静态代码块-&gt;初始化父类的属性值/父类的普通代码块(按照代码的顺序排列执行)-&gt;父类的构造方法-&gt;初始化子类的属性值/子类的普通代码块(按照代码的顺序排列执行)-&gt;子类的构造方法。构造方法最后才执行。 面试题 2面试官开始出第二题了，又抛来一段代码，细品： 1234567891011121314151617181920212223242526272829303132333435363738package com.example.demo;public class JVMClass2 &#123; public static void main(String[] args) &#123; Singleton1 s1 &#x3D; Singleton1.getSingleton(); Singleton2 s2 &#x3D; Singleton2.getSingleton(); System.out.println(&quot;s1:counter1 &#x3D; &quot;+ s1.counter1); System.out.println(&quot;s1:counter2 &#x3D; &quot;+s1.counter2); System.out.println(&quot;s2:counter1 &#x3D; &quot;+ s2.counter1); System.out.println(&quot;s2:counter2 &#x3D; &quot;+s2.counter2); &#125;&#125;class Singleton1&#123; private static Singleton1 singleton &#x3D; new Singleton1(); public static int counter1; public static int counter2 &#x3D; 0; public Singleton1()&#123; counter1++; counter2++; &#125; public static Singleton1 getSingleton()&#123; return singleton; &#125;&#125;class Singleton2&#123; public static int counter1; public static int counter2 &#x3D; 0; private static Singleton2 singleton &#x3D; new Singleton2(); public Singleton2()&#123; counter1++; counter2++; &#125; public static Singleton2 getSingleton()&#123; return singleton; &#125;&#125; 大家可以开动脑经，结合上面的加载顺序，来分析分析这道题的答案。我们直接看运行结果 是不是和内心预期的有点出入啊，我们按照上面的思路分析下， 构造器是在所有初始化之后才执行的 那么在所有属性初始化之后，在执行构造器的时候，counter1 和 counter2 都会有默认的初始值 0，那么执行完构造器之后，counter1 和 counter2 结果肯定都是 1， 如果这样的话，Singleton2 和 Singleton1 的执行结果应该是一样的，那么为什么 Singleton2 和我们的预期一致，Singleton1 却有出入呢 我们看到他们俩唯一的区别是进行 new 实例化的位置不一样。 既然找到了不同，我们继续往下分析，我们主要分析和我们预期不一致的 Singleton1。 Singleton1 实例化是在静态变量位置上面，所有优先他们执行，所以进入构造器执行，同学们可能会有疑问了，上面不是说构造器是在最后才执行么，为什么这里会先执行呢？因为这个构造器执行是有实例化代码触发的，所以会进行内部递优先执行构造方法 进入构造放方法，这时候 counter1 和 counter2 还是保持零值（这是有虚拟机加载 class 准备阶段执行的，后面会说），执行++之后，counter1 和 counter2 都会变成 1。构造方法执行结束，进入下一步 counter1 没有被显示赋值，counter2 被重新赋值为 0，到此初始化结束。 得到 counter1 和 counter2 分别为 1 和 0 到此，面试已经结束，我们现在来结合理论知识来总结下 jvm 类加载和初始化顺序。 java 类加载分为五个过程，如图： 类加载顺序加载验证 文件格式的验证： 验证文件格式是否按照虚拟机的规范，也就是我们前面 class 文件结构中的内容，比如这是不是一个 Class 文件（看魔数，是否位 CAFEBABE）；Java 版本是否符合当前虚拟机的范围（Java 可以向下兼容，但是不能处理大于当前版本的程序）等等。 元数据的验证： 对 Class 文件中的元数据进行验证，是否存在不符合 Java 语义的元数据信息。这里有的朋友可能会比较疑惑，什么是元数据呢？一般情况下，一个文件中都数据和元数据。数据指的是实际数据，而元数据（Metadata）是用来描述 数据的数据。用过 Java 注解的朋友应该对元数据这种叫法并不陌生，对应的元注解，其实说的差不多都是一个意思。举个例子：比如说我们定义了一个变量 int a = 1；可以理解成数据就是 1，而元数据就是描述有一个字符串变量“a”，这个“a”的类型是 int 型的，它的值也是一个 int 型的 1，这就是描述数据的数据，就是元数据。 字节码的验证： 通过数据流和控制流分析，来确定程序语义是否合法。以数据来说，要保证类型转换是有效的；对于控制流程的代码，不能让指令跳转到其它方法的字节码指令上等…… 符号引用的验证： 为了保证解析动作能正常完成，还需在虚拟机将符号引用转成直接引用的时候，判断其它要引用的类是否符合规定。比如，要引用的类是否能够被找到；引用的属性在对应类中是否存在，权限是否符合要求（private 的是不能访问 的）等。 准备解析初始化卸载了解完加载的五大步之后，我们再看下最后一个重要的知识点，就是我们使用类的时候，什么时候会触发类的初始化呢，分为以下四种情况： 什么时候会进行类的初始化 当遇到 new 、 getstatic、putstatic 或 invokestatic 这 4 条直接码指令时，比如 new 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。 使用 java.lang.reflect 包的方法对类进行反射调用时 ，如果类没初始化，需要触发其初始化。 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类。 当使用 JDK1.7 的动态动态语言时，如果一个 MethodHandle 实例的最后解析结构为 REF_getStatic、REF_putStatic、REF_invokeStatic、的方法句柄，并且这个句柄没有初始化，则需要先触发器初始化。 今天分享的面试内容到此结束，我们下一期再见吧。文末附赠视频教程和面试资料。 微信搜一搜【乐哉开讲】关注帅气的我，回复【干货】，将会有大量面试资料和架构师必看书籍等你挑选，包括 java 基础、java 并发、微服务、中间件等更多资料等你来取哦。书读的越多而不加思考，你就会觉得你知道得很多；而当你读书而思考得越多的时候，你就会越清楚地看到，你知道得很少。——伏尔泰","categories":[],"tags":[]},{"title":"并发编程｜连小白都能听懂的“synchronized”关键字讲解，面试官直呼好！","slug":"yuque/并发编程｜连小白都能听懂的“synchronized”关键字讲解，面试官直呼好！","date":"2021-02-28T14:22:29.000Z","updated":"2021-02-28T14:42:47.658Z","comments":true,"path":"post/4cac77a1.html","link":"","permalink":"https://yangletec.github.io/post/4cac77a1.html","excerpt":"","text":"synchronized 有哪几种使用方式？ 普通同步方法，锁是当前实例对象 静态同步方法，锁是当前类的 class 对象 同步代码块，锁事 synchronized 括号中的对象 jvm 怎么标识线程已经获取锁呢？这得要说说 java 对象头了 存储对象的 hashcode 或锁信息 存储到对象类型数据的指针 数组长度(如果当前对象是数组) Mark Word Class Metadata Address Array length 注意：锁可以升级但是不能降级 什么是偏向锁？ 偏向锁的获取 线程 A 进入临界点前，判断对象头中 Mark Word 中是否存着指向本线程的偏向锁，如果有，则直接获取锁成功，进入临界区，如果不存在，则执行下一步 Mark word 的偏向锁表示位是否设置成 1(当前已经是偏向锁)，如果没有，则使用 CAS 竞争锁，否则 使用 CAS 将对象头的偏向锁指向当前线程 偏向锁的撤销 锁撤销只会发生在锁竞争的时候并且当前没有正在执行的字节码 暂停拥有偏向锁的线程 检查拥有偏向锁的进程是否存活，如果不处于存活状态，则将对象头设置为无锁状态，否则 如果线程一直存活，则拥有偏向锁的栈会被执行，栈中的锁记录和对象头的 Mark word 要么重新偏向于其他线程，要么恢复到无无锁状态，最后唤醒暂停的线程 关闭偏向锁偏向锁在 jdk 中默认是启用的，如果你确定你的线程中会发生锁竞争，那么你可以使用如下命令关闭偏向锁-XX:BiasedLockingStartupDelay = 0; 轻量级锁 轻量级锁加锁 线程在执行同步代码块之前，JVM 会在当前的栈桢中创建存储锁记录的空间，并将锁对象头的 Mark Word 内容复制到刚才创建的锁记录空间中，这一步叫做“Displaced Mark Word”， 然后当前线程尝试使用 CAS 将对象头中的 Mark Word 替换为指向当前锁记录空间的内存地址， 如果替换成功则成功获取锁，否则表示有其他线程在竞争这把锁，则 使用自旋的方式来获取锁。 轻量级锁解锁 使用 CAS 操作将获取锁之前复制的 Mark Word 替换回锁对象头中，如果成果成功，则表示没有竞争发生， 如果失败，锁就会膨胀成重量级锁，并且在使用自旋方式尝试获取轻量级锁的线程会被阻塞 拥有轻量级锁的线程释放锁并唤醒等待的线程，被唤醒的线程重新争夺锁，访问临界区。 为什么锁不能被降级锁的优缺点对比 锁 优点 缺点 使用场景 偏向锁 加锁和解锁不需要额外的消耗 如果线程间存在竞争，则需要额外消耗 只有一个线程的场景 轻量级锁 竞争的线程不会阻塞，提高程序响应速度 自旋操作，时间过长会消耗过多的 cpu 追求响应时间，同步块中的任务执行的很快，不会长时间阻塞 重量级锁 线程竞争不使用自旋，不会消耗过多 cpu 线程阻塞，响应时间慢 追求吞吐量，同步块执行时间长 微信搜一搜【AI 码师】关注帅气的我，回复【干货】，将会有大量面试资料和架构师必看书籍等你挑选，包括 java 基础、java 并发、微服务、中间件等更多资料等你来取哦。书读的越多而不加思考，你就会觉得你知道得很多；而当你读书而思考得越多的时候，你就会越清楚地看到，你知道得很少。——伏尔泰","categories":[],"tags":[]},{"title":"分布式｜玩转redis看这一篇就够了，轻轻松松应对各种面试难题","slug":"yuque/分布式｜玩转redis看这一篇就够了，轻轻松松应对各种面试难题","date":"2021-02-28T14:21:40.000Z","updated":"2021-02-28T14:42:47.874Z","comments":true,"path":"post/1aefe554.html","link":"","permalink":"https://yangletec.github.io/post/1aefe554.html","excerpt":"","text":"数据存储类型介绍业务数据的特殊性作为缓存使用 原始业务功能设计 秒杀 618 活动 双 11 活动 12306 附加功能 系统功能优化升级 单服务器升级集群 session 管理 Token 管理 Redis 数据类型(5 种常用) string string hash HashMap list LinkedList set HashSet sorted_set TreeSet string get key set key del [key] mset key1 value1 key2 value2 mget key1 key2 strlen key append key value 单数据操作与多数据操作的选择之或？ 比较发送与执行时间消耗时间对比 string 数据类型的扩展操作 设置数值数据新增加指定范围的值，可以使用负号作为递减操作 123incr keyincrby key incrementincrbyfloat key increment 设置数值数据新减去指定范围的值 12decr keydecrby key increment 设置数值有效期 解决时效性控制的操作 12setex key second value 秒psetex key milliseconds value 毫秒 string 类型数据操作时的注意事项 设置数值有效期 解决时效性控制的操作 数据未获取到 1（nil）等于 null 数据最大存储量 512m 数据计算最大范围 (java 中的 long 的最大值) 9223372036854775807 string 类型应用场景 在 redis 中存储用户信息 user333:fans -&gt; 11111 数值 user333 -&gt; {id:333,fans:333} json redis 应用于各种结构性和非结构性高热度数据访问的加速 key 的设置约定 表名:主键名:主键值:字段名 hash 底层使用哈希表结构存储hash 存储结构优化 如果 field 字段数量少，使用数据数组进行存储 如果 field 字段数量多，存储结构使用 hashmap hash 类型数据的基本操作 添加/修改数据 1hset key field value 获取数据 12hget key fieldhgetall key 删除数据 1hdel key field [field2] 添加/修改多个数据 1hmset key field1 value1 field2 value2 获取多个数据 1hmget key filed1 field2 获取 hash 表中字段的数量 1hlen key 获取 hash 表中是否存在指定字段 1hexists key field hash 数据类型的扩展操作 获取 hash 表中所有的字段名和值 12 hkeys keyhvalues key 设置值自增 12 hincrby key incrementhincrbyfloat key increment 设置值字符，存在则设置失败，不存在则设置成功 1setnx key field value hash 数据类型注意事项 hash 类型 value 只能存字符串 每个 hash 可以存储 2^32-1 个键值对 hash 可以灵活操作属性，虽然可以作为对象使用，但是不能滥用 hgetall key 获取 hash 内的键值对 hash 数据类型应用场景 购物车 listlist 类型数据的基本操作 链表 添加/修改数据 12 lpush key value [value]rpush key value [value] 获取数据 123 lrange key start stop (0,-1获取全部)lindex key indexllen key 获取并移除数据 12 lpop keyrpop key 规定时间内获取并移除数据 12 blpop key [key] timeout (单位为秒)brpop key [key] timeout (单位为秒) 业务场景 删除指定元素 1lrem key count value list 数据类型注意事项 list 数据保存的数据都是 string 类型，最多 2^32-1 个元素 list 常被用作队列 list 结束索引为-1 list 可以进行数据分页操作 第一页来自 list，第二页及更多来自数据库 业务场景 信息管理 多路信息汇总 最新消息 set 无序set 类型数据的基本操作 添加数据 1sadd key memeber1 [member] 获取全部数据 1smembers key 删除数据 1srem key member1 [member] set 类型数据的扩展操作 随机获取几个元素 1srandmember key count 随机获取几个元素 并删除 1spop key count 获取元素个数 1scard key 求两个集合的交并差 123sinter key key2sunion key1 key2sdiff key1 key2 求两个集合的交并差 并存到指定集合 123sinterstore desc key key2sunionstore desc key1 key2sdiffstore desc key1 key2 将指定数据从原始集合移动到目标集合中 1smove source desc member 查看数据是否存在 1sismember key memeber 业务场景 随机推荐和抽奖等 zset 有序zset 类型数据的基本操作 添加数据 1zadd key score memeber1 查询 1zrange key start stop [withscores] 查询 反序 1zrevrange key start stop [withscores] 删除 1zrem key memeber [member] 数据类型实战 限流 如果限流为 100 则把最大值-100，利用异常做限制，不需要每次进行判断 通用命令 删除 key del key key 的类型 type key key 是否存在 exists key 为 key 设置有效期 1234 expire key secondspexpire key millseconds expireeat key timestamp pexpireeat key mill-timestamp 获取 key 有效时间 12ttl keypttl key 切换 key 从临时转为永久 1persist key 查询模式 123keys * 匹配所有keys ?lezai 匹配单个字符keys a[abc] 匹配aa ab ac key 重命名 12rename key new keyrenamenx key newkey 新key不存在则成功 对所有 key 里面的内容排序 1sort key 切换数据库 12select dbindexping pong 数据库移动 123mv key dbindexflushdb 删除当前库数据flushall 清除所有数据 Jedis 客户端连接 redis 123连接 Jedis jedis &#x3D;new Jedis(&quot;127.0.0.1&quot;,6379)设置 jedis.set(&quot;name&quot;,123)获取 jedis.get(&quot;name&quot;) Linux 安装 redis 下载 wget http://download.redis.io/releases/redis-4.0.10.tar.gz 解压 tar-xvzf redis-4.0.10.tar.gz 编译 make 安装 make install 指定端口启动 redis-server –port 9999 指定端口连接 redis-cli -p 9999 指定配置文件启动 redis-server conf/redis.conf redis 服务端基本配置 daemonize yes 以守护进程方式启动 port 6379 设置启动端口 dir “/redis/data” 设置当前服务文件保存位置 logfile “744.log” 设置日志文件名 持久化RDB save 手动执行保存一次快照信息 相关配置 123dbfilename dump.rdb 设置本地数据库文件 默认为dump.rdb 一般设置为 dump-端口号.rdbrdbcompression yes 是否启动压缩rdbchecksum yes 是否对库文件进行校验 bgsave 使用 fork 函数生成一个子进程，在子进程中执行 rdb 自动启动定时保存 1234配置信息 save second changenumsave 900 1 900 秒内有一个key变化save 60 3 60秒内有3个key变化save 10 6 AOF写数据的三种策略 always 每次操作都同步写入到 aof 中，数据零误差 性能低 everysec 每秒 将缓冲区的指令同步到 aof 中，系统宕机只会丢失一秒的数据 no 系统控制 不可控 AOF 功能开启 appendonly yes | no 开启持久化 默认不开启 appendfsync always | everysec | no AOF 策略 appendfilename fielname AOF 重写 重写规则 进程内已超时的数据不再重写 忽略无效指令 对同一数据的多条写合并为一条 bgrewriteaof 重写 aof 指令 AOF 与 RDB 选择 数据很敏感 使用 aof 数据间断性呈现 RDB 综合方案 同时开启 rdb+aof Redis 高级操作Redis 事务事务的基本操作 开启事务 1multi 设定事务的开启 执行指令后，全都加入到事务中 执行事务 1exec 设定事务的结束，同时执行事务，与multi成对出现 取消事务 1discard 取消事务 注意 1事务出错不能回滚，产生的数据不会回滚 锁 watch key1 key2 … 1监控某个key，需要在事务外面使用，如果key被外部修改了，则终止事务的执行 unwatch 1取消对事务的监控 分布式锁 setnx key value 解决死锁 添加时效性Redis 删除策略定时删除 创建一个定时器，当 key 设置有过期时间，且过期时间到达，由定时器执行对键的删除操作 牺牲 CPU 保证内存空间 CPU 空闲 内存紧张 惰性删除 在获取数据的时候，如果未过期，返回数据，过期了，删除，返回不存在 牺牲存储保证 CPU 存储空间足 CPU 紧张 定期删除 定期删除策略是怎么实现的？通过 activeExpireCycle 函数，serverCron 函数执行时，activeExpireCycle 函数就会被调用，规定的时间里面分多次遍历服务器的 expires 字典随机检查一部分 key 的过期时间，并删除其中的过期 key例如 Redis 每秒处理： 测试随机的 20 个 keys 进行相关过期检测。 删除所有已经过期的 keys。 如果有多于 25%的 keys 过期，重复步奏 1. Redis 逐出算法 在执行每一个命令前，都调用 freememoryifneed,内存不足，需要临时删除一些数据清理空间 maxmemory 最大内存 maxmemory-samples 随机选择 maxmemory-policy 有效期的数据 1volatile-lru 最近最少使用 1volatile-lfu 最近使用次数最少 1volatile-ttl 挑选将要过期的数据 1volatile-random 任意选择数据淘汰 所有数据 1allkeys-lru 最近最少使用 1allkeys-lfu 最近使用次数最少 1allkeys-random 任意选择数据淘汰 no-enviction 高级数据类型bitmaps setbits key offset value getbits key offset bitop [and or not xor] destkey key1 key2 对多个 key 组合操作，保存到 destkey 中 bitcount key [start end] 统计为 1 的数量 HyperLogLog pfadd key element .. pfcount key pfmerge destkey sourcekey1 sourcekey2 相关说明 1用于基数统计，不是集合，不保存数据，只记录数量而不是具体数据 1核心是基数估算，最终数值存在一定误差 1误差范围：0.81% 1占用12k 1pfadd 不是一次性分配12看，而是随基数增大而增加空间 Geo 距离计算 geoadd key longitude latitude member … 添加坐标 geopos key member … 获取坐标 geodist key member1 member2 unit 获取坐标点距离 georadius key longitude latitude radius unit 根据坐标求范围内的数据 georadiusbymember key memeber radius unit 根据点求范围内的数据 Redis 集群主从复制简介互联网“三高” 高性能 高并发 高可用 5 个 9 99.999% 工作流程 建立连接 数据同步(复制数据) 反复同步(命令传播) 建立连接阶段工作流程 11. 发送指令 slaveof ip port 12. 接收到指令，响应对方 13.保存master的ip与端口 masterip masterport 14.根据连接保存的信息创建与master的socket 15.向master周期发送ping，master相应pong 16.slave 发送author password 17.验证授权 18.发送指令replyconfiglistening-port port-num 19.保存slave端口号 连接的三种方式 11.slaveof if ip port 12.slave启动时 带参数 --slaveof masterip masterport 13.服务器配置 slaveof if ip port 数据同步阶段 1全量复制开始 1①发送指令 psync2 请求同步数据 1②master执行bgsave 1③第一个slave建立连接，创建命令缓冲区 1④生成rdb，通过socket发给slave 1⑤vslave接受rdb，清空数据，执行rdb文件恢复 1⑥发送命令，告知已经恢复完成 1全量复制结束 1部分复制开始 1⑦发送复制缓冲区指令信息 1⑧接受指令，执行bgrewriteeaof，恢复数据 1部分复制结束 命令传播阶段 1复制缓冲区 偏移量 字节值 1工作原理： 通过offset区分不同的slave当前数据传播的差异，maste记录已发送的信息对应的offset，slave记录已经接受的offset 数据同步(全) slave 发送指令 psync ？-1 master 收到发现没有偏移量，则使用 bgsave 生成 rdb 文件，记录当前的复制偏移量 offset 发送 fullresync runid offset，通过 socket 发送给 slave，期间收到客户端命令，offset 发生变化 收到 + fullresync 保存 master 的 runid 和 offset，清空当前数据，接受 rdb 文件，执行恢复 发送命令 psync2 runid offset master 接受到命令，判定 runid 是否匹配，判定 offset 是否在缓冲中 如果 runid 和 offset 不匹配，则执行全量复制 如果 runid 和 master 校验通过，offset 和 offset 相同，忽略 如果 runid 和 master 校验通过，offset 和 offset 不相同， 发送 continue offset，通过 socket 发送缓冲区中 offset 到 offset 的数据 slave 接收到 continue，保存 master 的 offset，接受信息后，执行 bgrewriteaof,恢复数据 主从复制参数注意 1① slave-server-stable-data yes|no 同步过程，开启或关闭对外写操作 1②repl-backlog-size 1mb 设置master指令缓冲区大小， 主从复制常见问题频繁全量复制 1①缓冲区太小 导致offset不匹配 解决:缓冲区设置大点 1②maset宕机重启，导致runid和offset丢失，全量复制 master执行shutdown 时，将runid和offset进行保存，重启时进行读取 频繁网络中断 1① 执行命令一直阻塞耗时 设置超时时间 数据不一致 哨兵简介 哨兵是一个分布式系统，用于对主从结构中的每台服务器做监控，当出现故障通过投票随机选择新的 master 并将所有的 slave 连接到信息 master 作用 监控 通知 自动故障转移 启用哨兵模式 启动哨兵 1redis-sentinel redis-sentinel.conf 1配置文件中配置 主redis节点 哨兵工作原理监控阶段 用于同步各个节点的状态信息 获取 sentinel 的状态(是否在线) 获取 master 状态 获取所有 slave 状态 master 保存 sentinel 信息，作用是让其他 sentinel 来发现其他的 sentinel，建立订阅通道 通知阶段 每个 sentinel 进行互通 故障转移阶段 sentinel 发现 master 关挂了，标记 master 为主观下线，然后发送命令给奇特哨兵，其他哨兵也会去看看状态，如果一半以上的哨兵认为 master 挂了，那就直接标记为客观下线，执行故障转移操作 sentinel 通过竞选，然后获得此次处理 master 的权利，成为领头 sentinel 去除不在线的 去除响应慢的 与原 master 断开时间久的 优先原则 优先级 offset runid 发送指令 向新的 master 发送 slave no one 指令 向其他 slave 发送 slaveof 新的 ip 端口 集群简介数据存储设计 通过算法设计，计算出 key 应该保存的位置 将所有的存储空间分割成 16384 份，每台主机保存一部分代表的是一个存储空间，不是一个 key 的保存空间 将 key 按照计算的结果放到对应的存储空间 集群内部通讯设计 各个节点相互通信，保存各个库中槽的编号数据 一次命中，直接返回 一次未命中，告知具体位置 环境搭建 cluster-enabled yes cluster-config-file node-6379.conf cluster-node-timeout 10000 在 src 下执行 1.&#x2F;redis-trib.rb create --replicas 1(一个master,一个slave,2 一个master两个slave) 主[...] 从[...] 连接集群客户端 1redis-cli -c 企业级解决方案缓存预热 系统启动前，提前将相关的缓存数据直接加载到缓存系统，避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题，用户直接查询事先被预热的数据 缓存雪崩 缓存雪崩就是瞬间过期数据量太大，导致对数据库服务器造成压力，如能够有效避免过期时间集中吗，可以有效解决雪崩现象的出现，配合其他策略一起使用，并监控服务器的运行数据，根据运行记录做快速调整 缓存击穿 单个高热数据过期的瞬间，数据访问量比较大，未命中 redis，引发了大量对同一数据的数据库访问，导致对数据库服务器造成压力，对应策略应该业务数据分析与预防上进行，配合运行监控测试与即时调整策略，毕竟单个 key 过期监控难度高，配合雪崩处理策略即可，定时刷新、二级缓存、加锁 缓存穿透 访问不存在的数据，跳过了合法数据的 redis 数据缓存阶段，每次访问数据库，造成对数据库的压力 解决方案 缓存 null key 加密 实时监控 白名单策略 bitmaps(效率低)","categories":[],"tags":[]},{"title":"源码阅读｜怒肝了9道 HashMap经典面试题，需要的快速来取（不包邮哦）","slug":"yuque/源码阅读｜怒肝了9道 HashMap经典面试题，需要的快速来取（不包邮哦）","date":"2021-02-28T14:21:00.000Z","updated":"2021-02-28T14:42:48.038Z","comments":true,"path":"post/10bc2040.html","link":"","permalink":"https://yangletec.github.io/post/10bc2040.html","excerpt":"","text":"背景 促使自己开始研究源码的原因主要有两个，第一个是在面试高级工程师的时候，源码这块是必问的，第二个原因是现在框架是越来越多，也没有太多精力去学习，于是就准备开始研究各种底层知识，看看那些底层大佬们是如何写代码，这是踏出的第一步，后面会有越来越多的源码学习经验和大家一起分享，希望大家能够提出宝贵的意见。话不多说，直接进入我们今天的主题 开发环境 开发工具 JDK 版本 IDEA 2020 JDK1.8 抛砖引玉首先给大家呈上几道经典的有关 hashmap 1.8 的面试题？ HashMap 的初始容量为什么是 2 的幂次方？ HashMap 在什么时候会进行扩容？ HashMap 是如何进行扩容的？ HashMap 底层数据结构？ HashMap1.8 为什么引入红黑树？ HashMap 什么时候会将链表转换成红黑树？ HashMap 在多线程情况下会出现什么？ 能说说 HashMap 的 hash 算法么？ HashMap 是如何定位到 key 所在数组上的位置的? 先说这么多吧，相信大家应该都会被问过这些问题，会不会很惊讶，就一个 hashmap 都能整出这么多面试问题？接下来我会通过本篇文章带着大家一起解读 hashmap 的这些骚操作，大家看完之后，上面的这些面试题都会知道该如何解答了，我们开始吧~ 在讲代码之前我想先和大家说下 hashmap 里面的一个数据结构 首先 hashmap 底层是一个数据结构，为什么要用数组呢，因为他查找非常的快，于是刚开始他长这样,他的初始长度是 16 然后我插入一个 key，他是怎么计算到自己的位置的呢，通过计算他的 hash 码，得到一个整数，然后和 16 取模，就能够将数据散列到 0-15 的位置了啊，但是 jdk 会用一个更加牛逼的方法去算出这个位置，后面我会说到的，看完之后，你会觉得算法真香。 当有越来越多的数据存进来之后，发现我的那个位置被人占用了，那可咋办呢，我又不能覆盖它吧，然后就有了链表这个新的成员加入，先看下链表和数组的结合 也就是说当位置相同的时候，所有的数据都会以链表的形式在那个位置一直往下接，就形成了上面这样的形式 加入链表之后，我们的数据存储问题是解决了，但是当这个链表越来越长的时候，我们找起来就费劲了，我们知道链表结构增加和删除是很快的，但是查找的复杂度就是 o(n)了，得挨个遍历。所以有必要引入新的成员了，红黑树 红黑树是平衡二叉树的一种实现方式，数据结构这块后续会有相关的文章进行讲解，那我们就来看下引入红黑树之后，是怎样一个组合呢好的，hashmap 结构这块我已经和大家大概的讲完了，下面就让我们一步一步分析代码，来解决我们心中的疑惑吧！ 代码示例一个简单地 main 方法，然后跟着这个方法，我们调到 map 的世界里面去 1234567public class Main &#123; public static void main(String[] args) &#123; // write your code here Map&lt;String,String&gt; map = new HashMap&lt;&gt;(27); map.put(&quot;name&quot;,&quot;乐哉开讲&quot;); &#125;&#125; 我们先进入到 new HashMap，看看构造器都给我们做了什么 123public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125; 可以看到他又调用了另外一个构造方法，并且又调用了另外一个构造方法DEFAULT_LOAD_FACTOR 这个就是一个负载因子 0.75 123456789101112public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; 我们来分下这段代码， 判断我们设置的初始容量是否合法 判断初始容量是否大于最大容量 MAXIMUM_CAPACITY = 1 &lt;&lt; 30;一般都不会大于这个最大值得，如果大于，就用这个最大值作为初始容量 判断负载因子是否合法，后面讲扩容得地方的时候再说这个负载因子是用来干嘛的 tableSizeFor 是为了计算出 大于等于这个初始容量的最小二次幂，如 15 的最小二次幂为 16 7 的最小二次幂是 8，看下具体是如何实现的，有兴趣的话可以跑下这段代码，看看是不是这样的，所以最终 map 的初始容量都是 2 的幂次方，并不一定是我们设置的数值 123456789static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 构造器分析完之后，我们在来到开始的地方，执行 put 方法，我们点进去看下，这里面是我们这次讲的核心的地方，大家认真看下 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; 我们看到 put 方法是去调用 putVal 方法去执行 put 逻辑的，先不跟进去看，我们会看到，这里会将 key 做一个 hash 运算，看看上面的面试题 8，是不是也说到这个了，我们就点进去看下，这个 hash 他做了什么？ 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 大家会不会很失望，这里面只有三行代码，能做什么呢？虽然只有只有三行代码，但是这里减小了 hash 碰撞的机会，什么叫 hash 碰撞呢，就是我们刚开始的时候提到的有些数据得到了相同的下标，然后会以链表的形式存储，会导致链表过长，这里就是为了让 hash 的更加均匀，而采取的一些手段，我们来分析下代码 key 如果为空的话，直接返回 hash 为 0 key 进行 hashcode 的话，会得到一串整数， 我们知道整形是占用四个字节，占用 32 个 bit，我们将前 16 个作为高位，后 16 个作为低位，然后将 32 个 bit 右移 16，是不是就能得到高 16 位的值，然后再讲高位和低位进行疑惑，得到一个新的二进制，为什么这么做呢，因为这样能够在计算元素下标的时候，能够让 hash 的高位和低位都能参与进行来，减少碰撞的概率 返回新的 hashcodehashcode 计算完之后，我们再回到上一个方法 putVal 123456789101112131415161718192021222324252627282930313233343536373839404142final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 数组初始化 这个方法是很长的，大家需要认真的慢慢的看完，我们从第一行开始进行解析 5. 首先定义一个 Node 节点，和一个 Node 数组 6. 接着判断 table 是否为空或者数组长度是否为 0 ，这个 table 是什么呢，这个 table 就是存放你创建过得 Node 数组，如果你第一次 put 操作的话，这个就是空的，第二次进来就是有内容的 7.假如我们是第一次进来，他会给我们进行 resize 操作，也就是初始化一个长度的 Node 数组，我们点进去看下，他都做了什么？顺便说下，当数组进行扩容的时候也会进入到这个 resize 方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 这个代码写的真长。。。，不过没关系，我们今天就是来一探究竟的，我们一步一步往下看 7. 首先把 resize 之前的 Node 数组复制给 oldTab,保存旧的数组长度和旧的阈值(负载因子*数组长度) 8. 接着判断旧的数组容量是否是大于 0（扩容的话会进入这个判断条件），如果是大于 0 的话就判断是否大于最大的容量 MAXIMUM_CAPACITY，如果大于的话，会给阈值赋值为这个最大容量，返回 oldtab，如果旧数组的容量没有超过这个最大容量，则进行两倍扩容，阈值也会进行扩容 9. 如果旧的阈值大于 0，则将旧的阈值作为新的数组的大小，这一步我理解的是第一次构造 map 的时候不是设置了一个初始容量，然后转换为了一个二次幂，这里就是用那个值来初始化一个 Node 数组 10.如果旧的阈值等于 0 的话，那就会使用 map 默认的初始容量 16 和负载因子 0.75 来计算数组的容量和阈值大小， 10. 为什么最后又要判断下 newThr == 0 呢，因为这个如果为的话，肯定是第一次初始化数组的时候，他这个阈值是没有计算的，所以需要重新计算下。 11. 到目前为止，新的数组已经扩展或初始画好了 12. 再次判断 if (oldTab != null) ，如果旧的数组不为空的话，说明就是扩容，如果为空则可以直接返回这个数组了，不为空的话，则需要进行扩容后的数据迁移的工作了数据迁移 13. 遍历所有旧数组中的元素 14. 判断当前元素是否为空，不为空才进行数据迁移 15. 接下来会有三个判断，作用分别为 判断是否是单个节点，判断是否是红黑树节点，判断是否是链表 16. 首先判断如果是单个节点，则 通过 e 的 hash 值和新的容量-1 进行与运算，会得到这个元素在新数组中的索引位置，还记得我们前面说过 jdk 用了一个比较厉害的定位元素位置的方法么，这里就是他的实现过程假如我们有一个数值，我们想让他在 0-15 中间进行散列，我们想到使用模运算 %16,这里给大家介绍另外一种方法，假如是 19 ，16 取模之后，会是 3，如果将 19&amp; （16-1）,计算之后也是 3 后者效率会更高的，所以 jdk 采用后面这种方法，更加高效。大家会不会跟困惑这是为啥呢，我在这位大家简单地介绍下： 首先我们知道，与运算的话必须是全部为 1 则为 1，如果要达到这样的效果的话，这个数值必须是 2 的 n 次方-1，肯定是所有 bit 为都为 1，这也就是为什么 map 要求数组容量必须是 2 的幂次方了。接下来我们拿到 1111 这样的数值之后和我们的 hash 进行运算11111000110011 &amp; 1111 这样运算之后得到的是 3111111111111111 &amp; 1111 这样运算之后得到的是 15 永远也不会超过 15，大家这下应该知道这个原&gt; 理了吧 再接着判断如果是红黑树的话，则进行红黑树的相关操作 最后再判断如果是链表的话，则进行遍历迁移 这里一个主要的操作是 给索引相同的元素进行均分 将元素的 hash 和老的数组长度进行与操作，如果为，说明他的高位为 1，与新的数组长度进行与之后，还是原来的结果，如果不为 0，则可以直接将索引下标加上旧的数组长度，然后将节点引到新的数组对应的索引下面，这里大家有可能很懵，说这么多到底是什么意思呢，我以画图的形式和大家说下吧 我们看到上面会有两个 table，分别是扩容前的数组和扩容后的数组 5. oldTab 数组上的第七个索引上，元素的 hash 分别为 7 和 15，7 &amp; （8-1）和 15 &amp; (8-1) 都得到的是7，所以存放到了 7 上面， 6. 现在进行扩容，数组长度扩大为 16，这时候如果直接拿两个 hash 值和新的数组长度进行与运算的话，会得到 7 和 15 两个位置，这样链表就会被这两个位置均分掉 7. 但是我们看代码，jdk 并没有这么做，他先判断 hash 和原先的数组长度进行与操作，之前一直是和数组长度减 1 做与操作，如果结果为 0，说明他在新的数组上面索引的位置还是和当前一样，则直接把数据放到新数组上，如果不为 0 ，则只需要把当前索引位置加上旧的数组长度即可，因为数组扩容长旧数组两倍的， 讲了这么多，数组初始化这块讲完了我们再回到前面的代码 123456789101112131415161718192021222324252627282930313233343536373839404142final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 第一个判断处理完之后，我们继续往下看， p = tab[i = (n - 1) &amp; hash]) == null 判断如果数组所在的索引位置上的数据如果为空，则直接 new 一个新的 Node 直接放在元素上即可 如果不为空，则继续往下走 如果当前节点不为空，并且它的 key 值和我们传入的 key 是相等的，则直接取出这个节点，直接在这个节点上进行操作，后面再说 如果节点是红黑树节点，则进行红黑树相关的操作 如果上面条件都不满足的，则说明是一个链表结构， 遍历链表里面的元素 如果在链表里面找到了 key 值相同的节点，则直接取出这个节点，不再遍历 如果已经遍历到链表的最后一个节点都还没有拿到的话，则需要创建新的节点 这里有两种情况，通过 binCount 进行判断，这个变量用来干嘛的呢，我们会看到我们每次进行链表节点的时候都会把这个进行自增，其实也就是记录这个链表的长度 如果比较发现 链表的长度已经大于 map 中定义的 TREEIFY_THRESHOLD - 1 的话，也就是 7，就会将链表转换为红黑树，将数据存到红黑树中，这里为什么要减掉 1 呢，其实这块也是面试官必问的，也就是我刚开始提到的一个面试题：什么时候链表会转换成红黑树？map 里面定义的是 8，这里减了 1.是因为在我们进行遍历链表之前，我们已经取出来了数组上面的第一个链表元素了，后面的遍历是基于这个元素的 next 进行遍历的，所以这里就需要将 TREEIFY_THRESHOLD -1 作为转换条件判断。 最后我们看下这段代码 1234567if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; 这个 e 也就是我们上面取出来的元素，如果判断不为空的话，则说明 map 中已经存在了这个元素，则只需要给他赋上新的值就好了，然后将旧值返回回去，最后将数组长度加一，如果是更新操作，则不会走到这一步，加一之后如果发现当前的数组中元素的长度如果大于阈值则进行扩容操作。 到这里终于把 map 中最重要的 put 操作讲完了，get 和 remove 操作大家可以按照这个思路自己去看下咯，刚开始的面试题也在文章里都有讲解到，还有一个多线程情况下，map 会出现什么问题，这个后续再说了，本文篇幅有点长。。。，文中有讲的不准确的地方，希望各位大佬指正","categories":[],"tags":[]},{"title":"趣味编程故事｜java进程占用cpu过高怎么办，别急我来帮你","slug":"yuque/趣味编程故事｜java进程占用cpu过高怎么办，别急我来帮你","date":"2021-02-27T09:43:33.000Z","updated":"2021-02-28T14:42:48.110Z","comments":true,"path":"post/4ac29acd.html","link":"","permalink":"https://yangletec.github.io/post/4ac29acd.html","excerpt":"","text":"tags: [elk, springboot]categories: 分布式专题 关注公众号“AI 码师”领取 2021 最新面试资料一份 【主演】运维小哥：小李 测试小姐姐：小红 开发人员：本色出演 【剧情】在一个阳光明媚的清晨，原本还在睡梦中的我，被小李（运维小哥）的电话给惊醒了。小李：线上告警出来一个问题，赶快看一下！我：啥问题啊？小李：xx 服务器 现在 cpu 已经 100%了，我刚才查了下是 java 进程占用的我：好的，我来看下 我赶紧打开我的电脑，连接上了 xx 服务器，使用 top 命令查了下，确实是 java 进程占用 cpu 最高，我大脑飞速旋转想到:这几天也没有对程序做什么大改动啊，怎么会出现这个问题呢； 于是我把本地代码切换到 master 分支，在本地跑了下发布到线上的代码，观察了一会，也没看见 cpu 占用很高啊，我就很纳闷了，然后又上服务器看了下，java 进程占用 cpu 还是比较高的，这是怎么回事呢？难道要拿出我的杀手锏？ 接下来场面就比较血腥了，前方高能预警，系好安全带 【高能预警】我拿出了杀手锏，江湖上失传已久的九阴真经（其实就是平时的笔记哈哈），在上面找了一些命令，然后就是接下来的场景了： 【步骤一】找到最耗 cpu 的进程 top 得到最占用 CPU 的进程为：39890 【步骤二】找到 39890 进程中最消耗 cpu 的线程 top -d 1 -Hp 39890 得到最耗 CPU 的进程为 39900 【步骤二】将 PID 转换为十六进制 我们拿到了进程 PID 为：39900我们需要将它转为十六进制，才能进行后续指令操作printf “%x\\n” 39900得到结果：9bdc 【步骤三】打印当前线程的栈信息，看看它到底在干嘛 jstack 39890 | grep ‘9bdc’ -C5 现在已经找到了线程的栈信息，我看到了我自己写的代码，我用红框标记出来了 【步骤四】带着疑问去找代码 我们根据栈信息，直接在我们项目上定位到了那段代码 我发现，这里有一个死循环，我擦，忘记加休眠时间了，导致一直在死循环跑呢，终于找到问题根源了，加上了休眠时间，然后本地测试了下没问题； 关键这是线上的问题，得要赶紧上线啊，但是也得测试呢，我不得不拨通了小红（测试小姐姐）的电话，嘀。嘀。嘀。，我心里也在滴滴滴，这么早打电话给她让她测试，免不了被训，我已经做好了心里准备了，咦，通了： 小红：干啥？（测试小姐姐总是这么彪悍） 我：有有有一个小小的线上问题(说话已经开始紧张了，生怕她开骂，哈哈)，我已经改好了，需要你测试下，然后抓紧上线。 小红：今天不是周末么，还要测试，让不让人睡觉了啊？ 我：我我。。。，小姐姐，你长得这么可爱，美丽，端庄，温柔，贤惠，就帮忙测试下么，不然我这个月绩效又要被扣了… 小红：看你这么诚实，这次就勉强帮你测试下吧，但是下次不要在周末给我发提测了。 我：好的呢，下…(对方已经挂断了电话…) 哈哈，不管咋样，现在已经没我的事了，我合上电脑，又进入了梦乡… 全剧终经历了一次线上 bug 排查，让我胆战心惊，不过这次也给我增加了不少经验，毕竟重拾了 九阴正经，让我内功又增强了；后期会发布更多在实战方面的工作经验，与大家分享，希望大家能够喜欢，现在我需要你们的一键三连，哈哈，下期见。","categories":[],"tags":[]},{"title":"5年老java开发，我看过哪些书，看过哪些视频，刷过哪些面试题，都在这里了","slug":"yuque/5年老java开发，我看过哪些书，看过哪些视频，刷过哪些面试题，都在这里了","date":"2021-02-27T09:40:32.000Z","updated":"2021-02-28T14:42:48.130Z","comments":true,"path":"post/2f39c1bc.html","link":"","permalink":"https://yangletec.github.io/post/2f39c1bc.html","excerpt":"","text":"1234tags: [面试,资料]categories: 面试--- 本篇文章没有任何水文，全是干货 作为 5 年开发工程师，拥有 5 年一线大厂的开发经验； 在大学就开始自学 java，在这期间有迷茫过，也想放弃过，但是最后都坚持了下来； 现在我把我自己一路走来珍藏的资料和学习方法都分享给大家，谨献给有需要的和准备涉及此行业的同学。 关注公众号“AI 码师”,公众号内回复“5 年”，可免费领取资料 电子书籍大概 2.5g；视频教程大概 7.3g； 电子书籍JAVA 入门必备 并发编程 分布式 微服务 数据库 阿里技术图册 数据结构与算法 2021 最新面试资料（四套） 视频架构师教程","categories":[],"tags":[]},{"title":"趣味编程｜手写一个集成多数据源mongodb的 starter","slug":"yuque/趣味编程｜手写一个集成多数据源mongodb的 starter","date":"2021-02-27T08:50:31.000Z","updated":"2021-02-28T14:42:48.198Z","comments":true,"path":"post/3475e752.html","link":"","permalink":"https://yangletec.github.io/post/3475e752.html","excerpt":"","text":"关注公众号“AI 码师”领取 2021 最新面试资料一份，公众号内回复“源码”，获取本项目源码 【前言】主演：老王(技术总监)，小码（本猿） 老王：小码啊，我们项目中需要使用到 mongodb，你集成下吧，完成了和我说下。 小码：好的，一会就给你弄好。 小码三下五除二的给集成好了，然后给老王汇报了。 小码：王哥，我已经把 mongodb 集成好了。 老王：好的，现在由于我们项目中会用到很多 mongo 数据库，你现在集成的 mongo 支持多数据源动态切换么？ 小码：这个，这个，啥叫多数据源动态切换啊？ 老王：就是在运行过程中，能够根据需要动态去连接哪个数据库，咱们项目需要支持多个特性，如果你对这个不太清楚的话，我给你一个思路，你可以考虑使用切面来实现，具体怎么弄，你自己研究下. 小码：好的，王哥。 小码想了很久，各种百度，终于找到了解决方案，花了一上午的时间，终于弄完了，又去给老王汇报了。 小码：王哥，现在项目中的 mongo 已经实现了多数据源了（哈哈，心里很自豪）。 老王：小伙子，很快嘛，不过现在又来一个任务，你需要把你集成的这个功能封装成一个 starter，另外一个项目也需要使用这个功能，你抽时间封装下吧。 小码：好的，王哥，保证完成任务 小码下去之后，就开始研究怎么去封装成一个 starter，下班之前弄好了，不过这次他没去找老王了，准备第二天再去，不然又得加班，哈哈！！！ 【正文】前面水了那么多，主要是给大家设置一种场景，让同志们知道为啥要去做这么一个功能，现在就直接进入正题了： 【springboot 集成 mongodb】 引入 mongodb 依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 配置 mongodb 连接信息，在 application.yml 中配置 12345678910# 设置了用户名和密码的连接spring: data: mongodb: uri: mongodb:&#x2F;&#x2F;用户名:密码@IP:PORT&#x2F;数据库?authSource&#x3D;$&#123;auth_db:用户认证数据库&#125;# 没有设置用户名和密码的连接配置spring: data: mongodb: uri: mongodb:&#x2F;&#x2F;IP:PORT&#x2F;数据库 写测试代码 我们创建一个接口，然后在接口方法中去操作 monog 库: 接口中，直接引入 MongoTemplate，就可以直接操作 mongo 了，这里对 mongo 如何使用不做过多介绍。 1234567891011121314151617181920&#x2F;** * Created by AI码师 on 2019&#x2F;4&#x2F;19. * 关注公众号【AI码师】领取2021最新面试资料一份（很全） * @return *&#x2F;@RequestMapping(&quot;&#x2F;home&quot;)@RestControllerpublic class HomeController &#123; @Autowired private MongoTemplate mongoTemplate; @PostMapping public String addData(@RequestParam(value &#x3D; &quot;name&quot;) String name,@RequestParam(value &#x3D; &quot;addr&quot;) String addr,@RequestParam(value &#x3D; &quot;email&quot;) String email)&#123; Student student &#x3D; new Student(); student.setAddr(addr); student.setName(name); student.setEmail(email); mongoTemplate.insert(student); return &quot;添加成功&quot;; &#125;&#125; 请求接口： 响应数据： 响应添加成功，我们看下数据库，是否添加上去了： 数据已经添加上去了，说明已经集成成功了，但这还是第一步，我们需要做的是支持多数据源，接下来我们一起来完成逼格更高的多数据源 mongo 吧。 【实现多数据源】实现思路先介绍下实现多数据源动态切换的思路： 首先通过 AOP 技术，在调用方法前后动态替换 mongo 数据源，这个主要是替换 mongo 中 mongodbfactory(SimpleMongoClientDatabaseFactory)值,每个 factory 都维护自己需要连接的库，如果在操作之前，替换该参数为自己需要操作的数据库 factory，操作结束又切换成原来的，不就可以实现动态切换数据源了么。 说完了思路，我们直接上代码吧 垒代码 添加 aop 依赖 12345&lt;!--引入AOP依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; 修改数据库连接配置 123456789101112# 设置了用户名和密码的连接spring: data: mongodb: uri: mongodb:&#x2F;&#x2F;用户名:密码@IP:PORT&#x2F;#?authSource&#x3D;$&#123;auth_db:用户认证数据库&#125;# 没有设置用户名和密码的连接配置spring: data: mongodb: uri: mongodb:&#x2F;&#x2F;IP:PORT&#x2F;#与上述配置，做了小小的改动，将操作的数据库名称替换成了#，用来做后续备用 创建切面 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package com.aimashi.dynamicmongo.config;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Pointcut;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.data.mongodb.core.MongoDatabaseFactorySupport;import org.springframework.data.mongodb.core.MongoTemplate;import org.springframework.data.mongodb.core.SimpleMongoClientDatabaseFactory;import org.springframework.stereotype.Component;import java.lang.reflect.Field;import java.util.HashMap;import java.util.Map;&#x2F;** * Created by AI码师 on 2019&#x2F;4&#x2F;19. * 关注公众号【AI码师】领取2021最新面试资料一份（很全） * @return *&#x2F;@Component@Aspectpublic class MongoSwitch &#123; private final Logger logger &#x3D; LoggerFactory.getLogger(MongoSwitch.class); @Autowired private MongoDatabaseFactorySupport mongoDbFactory; private final Map&lt;String, MongoDatabaseFactorySupport&gt; templateMuliteMap &#x3D; new HashMap&lt;&gt;(); &#x2F;&#x2F; 获取配置文件的副本集连接 @Value(&quot;$&#123;spring.data.mongodb.uri&#125;&quot;) private String uri; &#x2F;&#x2F; @Pointcut(&quot;@annotation(com.pig4cloud.pig.common.log.annotation.MongoLog)&quot;) @Pointcut(&quot;execution(public * com.aimashi.dynamicmongo.config.MongotemplteService.*(..))&quot;) public void routeMongoDB() &#123;&#125; @Around(&quot;routeMongoDB()&quot;) public Object routeMongoDB(ProceedingJoinPoint joinPoint) &#123; Object result &#x3D; null; &#x2F;&#x2F; 获取需要访问的项目数据库 String dbName &#x3D; (String) joinPoint.getArgs()[0]; Object o &#x3D; joinPoint.getTarget(); Field[] fields &#x3D; o.getClass().getDeclaredFields(); MultiMongoTemplate mongoTemplate &#x3D; null; try &#123; for (Field field : fields) &#123; field.setAccessible(true); Class fieldclass &#x3D; field.getType(); &#x2F;&#x2F; 找到Template的变量 if (fieldclass &#x3D;&#x3D; MongoTemplate.class || fieldclass &#x3D;&#x3D; MultiMongoTemplate.class) &#123; &#x2F;&#x2F; 查找项目对应的MongFactory SimpleMongoClientDatabaseFactory simpleMongoClientDbFactory &#x3D; null; &#x2F;&#x2F; 实例化 if (templateMuliteMap.get(dbName) &#x3D;&#x3D; null) &#123; &#x2F;&#x2F; 替换数据源 simpleMongoClientDbFactory &#x3D; new SimpleMongoClientDatabaseFactory(this.uri.replace(&quot;#&quot;, dbName)); templateMuliteMap.put(dbName, simpleMongoClientDbFactory); &#125; else &#123; simpleMongoClientDbFactory &#x3D; (SimpleMongoClientDatabaseFactory) templateMuliteMap.get(dbName); &#125; &#x2F;&#x2F; 如果第一次，赋值成自定义的MongoTemplate子类 if (fieldclass &#x3D;&#x3D; MongoTemplate.class) &#123; mongoTemplate &#x3D; new MultiMongoTemplate(simpleMongoClientDbFactory); &#125; else if (fieldclass &#x3D;&#x3D; MultiMongoTemplate.class) &#123; Object fieldObject &#x3D; field.get(o); mongoTemplate &#x3D; (MultiMongoTemplate) fieldObject; &#125; &#x2F;&#x2F; 设置MongoFactory mongoTemplate.setMongoDbFactory(simpleMongoClientDbFactory); &#x2F;&#x2F; 重新赋值 field.set(o, mongoTemplate); break; &#125; &#125; try &#123; result &#x3D; joinPoint.proceed(); &#x2F;&#x2F; 清理ThreadLocal的变量 mongoTemplate.removeMongoDbFactory(); &#125; catch (Throwable t) &#123; logger.error(&quot;&quot;, t); mongoTemplate.removeMongoDbFactory(); &#125; &#125; catch (Exception e) &#123; logger.error(&quot;&quot;, e); &#125; return result; &#125;&#125; 创建相关配置类 12345678910111213141516package com.aimashi.dynamicmongo.config;import org.springframework.data.mongodb.core.MongoTemplate;import org.springframework.stereotype.Service;&#x2F;** * Created by AI码师 on 2019&#x2F;4&#x2F;19. * 关注公众号【AI码师】领取2021最新面试资料一份（很全） * @return *&#x2F;@Servicepublic class MongotemplteService &#123; private MongoTemplate mongoTemplate; public &lt;T&gt; T save(String dbName, T var1) &#123; return mongoTemplate.save(var1); &#125;&#125; 1234567891011121314151617181920212223242526272829303132package com.aimashi.dynamicmongo.config;import com.mongodb.client.MongoDatabase;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.data.mongodb.core.MongoDatabaseFactorySupport;import org.springframework.data.mongodb.core.MongoTemplate;public class MultiMongoTemplate extends MongoTemplate &#123; private Logger logger&#x3D; LoggerFactory.getLogger(MultiMongoTemplate.class);&#x2F;&#x2F;用来缓存当前MongoDbFactory private static ThreadLocal&lt;MongoDatabaseFactorySupport&gt; mongoDbFactoryThreadLocal; public MultiMongoTemplate(MongoDatabaseFactorySupport mongoDbFactory)&#123; super(mongoDbFactory); if(mongoDbFactoryThreadLocal&#x3D;&#x3D;null) &#123; mongoDbFactoryThreadLocal &#x3D; new ThreadLocal&lt;&gt;(); &#125; &#125; public void setMongoDbFactory(MongoDatabaseFactorySupport factory)&#123; mongoDbFactoryThreadLocal.set(factory); &#125; public void removeMongoDbFactory()&#123; mongoDbFactoryThreadLocal.remove(); &#125; @Override public MongoDatabase getDb() &#123; return mongoDbFactoryThreadLocal.get().getMongoDatabase(); &#125;&#125; 添加测试接口 123456789101112131415&#x2F;** * Created by AI码师 on 2019&#x2F;4&#x2F;19. * 关注公众号【AI码师】领取2021最新面试资料一份（很全） * @return *&#x2F;&#x2F;&#x2F; dbName 为数据库名称 @PutMapping public String addDataByDynamic(@RequestParam(value &#x3D; &quot;dbName&quot;) String dbName,@RequestParam(value &#x3D; &quot;name&quot;) String name,@RequestParam(value &#x3D; &quot;addr&quot;) String addr,@RequestParam(value &#x3D; &quot;email&quot;) String email)&#123; Student student &#x3D; new Student(); student.setAddr(addr); student.setName(name); student.setEmail(email); mongotemplteService.insert(dbName,student); return &quot;添加成功&quot;; &#125; 请求接口：数据库名参数传了 ams1 请求响应：响应成功 我们看下数据库，发现在数据库 ams1 下面已经有了此数据： 我们将数据库名参数修改为：ams2，进行请求 发现数据源已经切换成功了。 到这里，大家有没有发现自己很牛逼了啊，不过本篇文章还没算完，现在虽然已经实现了动态切换数据源的功能，但是还只能在自己项目上用，别的项目需要使用，只能直接复制过去，我们接下来需要做一个更牛逼的事情：手写一个 starter 来封装这个功能，别人只需要引入依赖，即可开箱即用： 【整合到 starter 里面】 创建一个 maven 项目：dynamicmongo-starter 将如下文件拷贝到新项目中 创建自动装配文件 1234567891011121314151617181920212223package com.aimashi.dynamicmongo.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;&#x2F;** * Created by AI码师 on 2019&#x2F;4&#x2F;19. * 关注公众号【AI码师】领取2021最新面试资料一份（很全） * @return *&#x2F;@Configuration(proxyBeanMethods &#x3D; false)public class MongodbAutoConfiguration &#123; @Bean public MongoSwitch mongoSwitch() &#123; return new MongoSwitch(); &#125; @Bean public MongotemplteService mongotemplteService() &#123; return new MongotemplteService(); &#125;&#125; 新建 resources/META_INF/spring.factories 文件 12org.springframework.boot.autoconfigure.EnableAutoConfiguration&#x3D;\\ com.aimashi.dynamicmongo.config.MongodbAutoConfiguration 到这里 starter 已经编写完成，是不是很简单。。 【使用 starter】starter 已经编写好，我们只需要在项目中引入该依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.aimashi&lt;&#x2F;groupId&gt; &lt;artifactId&gt;dynamicmongo-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt; 然后在需要操作 mongod 方法的地方，引入：MongotemplteService 即可； 注意 MongotemplteService 里面的方法大家按需扩充，目前只写了一个，大家使用的时候，只需要把 mongoTemplate 里面的方法名写到 MongotemplteService 中，然后再去调用 mongoTemplate 里面对应方法即可。 【总结】很少写这么长的实践类文章，现在已经十一点半了，该休息了，后面会有更多文章和大家一起分享，希望大家能有所收获，晚安！","categories":[],"tags":[]},{"title":"工作经验｜lambada处理集合的常用10种实战骚操作，我都记录下来了","slug":"yuque/工作经验｜lambada处理集合的常用10种实战骚操作，我都记录下来了","date":"2021-02-27T06:54:08.000Z","updated":"2021-02-28T14:42:48.226Z","comments":true,"path":"post/465c9d.html","link":"","permalink":"https://yangletec.github.io/post/465c9d.html","excerpt":"","text":"关注公众号“AI 码师”领取 2021 最新面试资料一份，公众号内回复“源码”，获取本项目源码 最近在项目上面经常使用 lambada 表达式，但是总是记不住，一直都在百度，写完之后就忘记了，感觉很费时间；这次就花点时间，把一些常用的 lambada 处理集合的实例都保存了下来(去重，分组，求和，list 转 map 等等)，以后就不用到处找了，刚好也可以给同学们分享下；另外也把一些关于使用 lambada 时遇到的坑也给大家一起分享下，所有代码拿来即用！！！本文档持续更新… 实例演示商品实体 12345678@Data@AllArgsConstructorpublic class GoodInfo &#123; private String mallSource; private String skuNo; private int price; private int monthCount;&#125; 排序集合排序在项目中用的频率还蛮高，这里以按照销量排序为例 123456789101112List&lt;GoodInfo&gt; goodInfos &#x3D; Arrays.asList(); goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_1112312312&quot;, 199, 100000)); goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_23534231231&quot;, 399, 10)); goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_1110080098&quot;, 299, 100)); goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_412313123&quot;, 99, 10000000)); goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_354532431&quot;, 599, 1)); goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_1423124131&quot;, 499, 10)); &#x2F;&#x2F; 按照销量正序 从小到大排序 goodInfos.sort(Comparator.comparing(GoodInfo::getMonthCount)); &#x2F;&#x2F; 按照销量倒序 从大到小排序 goodInfos.sort(Comparator.comparing(GoodInfo::getMonthCount).reversed()); 取最大值/取最小值/求和12345678910111213141516List&lt;GoodInfo&gt; goodInfos &#x3D; Arrays.asList();goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_1112312312&quot;, 199, 100000));goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_23534231231&quot;, 399, 10));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_1110080098&quot;, 299, 100));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_412313123&quot;, 99, 10000000));goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_354532431&quot;, 599, 1));goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_1423124131&quot;, 499, 10));&#x2F;&#x2F; 获取最大销量 注意如果求最大值是在filter之后使用例如，goodInfos.stream().filter().max一定要判断filter后集合数量是否不为空，否则使用max的get方法会报错GoodInfo hotGoodInfo &#x3D; goodInfos.stream().max(Comparator.comparing(GoodInfo::getMonthCount)).get();&#x2F;&#x2F; 求最低价格商品GoodInfo lowPriceGoodInfo &#x3D; goodInfos.stream().min(Comparator.comparing(GoodInfo::getMonthCount)).get();&#x2F;&#x2F; 计算商品总价格int sum &#x3D; goodInfos.stream().mapToInt(person -&gt; person.getPrice()).sum();&#x2F;&#x2F; 求平均价格double avg &#x3D; goodInfos.stream().mapToInt(person -&gt; person.getPrice()).average().getAsDouble(); 遍历12345678910111213List&lt;GoodInfo&gt; goodInfos &#x3D; Arrays.asList(); goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_1112312312&quot;, 199, 100000)); goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_23534231231&quot;, 399, 10)); goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_1110080098&quot;, 299, 100)); goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_412313123&quot;, 99, 10000000)); goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_354532431&quot;, 599, 1)); goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_1423124131&quot;, 499, 10)); &#x2F;&#x2F; 遍历输出所有商品id goodInfos.forEach( goodInfo -&gt; &#123; System.out.println(goodInfo.getSkuNo()); &#125;); 实体集合 转 单个属性的集合往往在我们项目中会有这样的需求：我需要提取集合中某一个属性，然后组装成集合，通常做法是先创建一个字符串集合，然后遍历原始集合，取出数据，放到字符串集合中，虽然也能实现功能，但是不免太过于繁琐，现在使用一行 lambada 表达式即可搞定： 12345678910List&lt;GoodInfo&gt; goodInfos &#x3D; Arrays.asList();goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_1112312312&quot;, 199, 100000));goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_23534231231&quot;, 399, 10));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_1110080098&quot;, 299, 100));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_412313123&quot;, 99, 10000000));goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_354532431&quot;, 599, 1));goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_1423124131&quot;, 499, 10));&#x2F;&#x2F; 将list转为某个属性map 这里是把所有skuno全部取出来 作为集合List&lt;String&gt; skuNos &#x3D; goodInfos.stream().map(goodInfo -&gt; goodInfo.getSkuNo()).collect(Collectors.toList()); 实体集合 转 map 返回12345678910List&lt;GoodInfo&gt; goodInfos &#x3D; Arrays.asList();goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_1112312312&quot;, 199, 100000));goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_23534231231&quot;, 399, 10));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_1110080098&quot;, 299, 100));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_412313123&quot;, 99, 10000000));goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_354532431&quot;, 599, 1));goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_1423124131&quot;, 499, 10));&#x2F;&#x2F; 将list转为map，key 为商品idMap&lt;String, GoodInfo&gt; map &#x3D; goodInfos.stream().collect(Collectors.toMap(GoodInfo::getSkuNo, goodInfo -&gt; goodInfo)); 实体集合按照某个属性分组12345678910List&lt;GoodInfo&gt; goodInfos &#x3D; Arrays.asList();goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_1112312312&quot;, 199, 100000));goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_23534231231&quot;, 399, 10));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_1110080098&quot;, 299, 100));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_412313123&quot;, 99, 10000000));goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_354532431&quot;, 599, 1));goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_1423124131&quot;, 499, 10));&#x2F;&#x2F; 按照商品来源分组Map&lt;String, List&lt;GoodInfo&gt;&gt; map &#x3D; goodInfos.stream().collect(Collectors.groupingBy(GoodInfo::getMallSource)); 过滤数据（记得接收）12345678910111213141516171819List&lt;GoodInfo&gt; goodInfos &#x3D; Arrays.asList();goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_1112312312&quot;, 199, 100000));goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_23534231231&quot;, 399, 10));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_1110080098&quot;, 299, 100));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_412313123&quot;, 99, 10000000));goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_354532431&quot;, 599, 1));goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_1423124131&quot;, 499, 10));&#x2F;&#x2F; 过滤商品价格大于300的&#x2F;&#x2F; todo 过滤后一定要使用集合接收，否则等于没有过滤List&lt;GoodInfo&gt; collect &#x3D; goodInfos.stream() .filter(goodInfo -&gt; goodInfo.getPrice() &gt; 300) .collect(Collectors.toList());collect.forEach( goodInfo -&gt; &#123; System.out.println(goodInfo.getPrice()); &#125;); 去重(两种方法可选)方法一 set 去重1234567891011121314151617List&lt;GoodInfo&gt; goodInfos &#x3D; Arrays.asList();goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_1112312312&quot;, 199, 100000));goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_23534231231&quot;, 399, 10));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_1110080098&quot;, 299, 100));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_1110080098&quot;, 299, 100));goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_412313123&quot;, 99, 10000000));goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_354532431&quot;, 599, 1));goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_1423124131&quot;, 499, 10));&#x2F;&#x2F; 使用treeset 集合来实现去重，这里一定要使用集合接收，不然等于没有去重List&lt;GoodInfo&gt; goodInfos1 &#x3D; goodInfos.stream() .collect( Collectors.collectingAndThen( Collectors.toCollection( () -&gt; new TreeSet&lt;&gt;(Comparator.comparing(o -&gt; o.getSkuNo()))), ArrayList::new)); 方法二 map 去重123456789101112131415161718192021public static void main(String[] args) &#123; List&lt;GoodInfo&gt; goodInfos &#x3D; Arrays.asList(); goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_1112312312&quot;, 199, 100000)); goodInfos.add(new GoodInfo(&quot;tb&quot;, &quot;tb_23534231231&quot;, 399, 10)); goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_1110080098&quot;, 299, 100)); goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_1110080098&quot;, 299, 100)); goodInfos.add(new GoodInfo(&quot;jd&quot;, &quot;jd_412313123&quot;, 99, 10000000)); goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_354532431&quot;, 599, 1)); goodInfos.add(new GoodInfo(&quot;pdd&quot;, &quot;pdd_1423124131&quot;, 499, 10)); &#x2F;&#x2F; 使用map去重 List&lt;GoodInfo&gt; goodInfos2 &#x3D; goodInfos.stream() .filter(distinctByKey(goodInfo -&gt; goodInfo.getSkuNo())) .collect(Collectors.toList()); &#125; public static &lt;T&gt; Predicate&lt;T&gt; distinctByKey(Function&lt;? super T, ?&gt; keyExtractor) &#123; Map&lt;Object, Boolean&gt; seen &#x3D; new ConcurrentHashMap&lt;&gt;(); return t -&gt; seen.putIfAbsent(keyExtractor.apply(t), Boolean.TRUE) &#x3D;&#x3D; null; &#125; 遇到的坑坑一 报错信息：java.util.NoSuchElementException: No value present 解决方案：一般出现这个错都是在 filter 操作后面使用了 max/min 等操作，然后调用了 get 方法，取不到数据，导致报错，所以建议检查 filter 操作后时候还有数据，有数据再进行后续操作： 1234 Optional&lt;User&gt; optional &#x3D; goodInfos.stream().max(userComparator);if(optional !&#x3D; null &amp;&amp; optional.isPresent()) &#123; recentUserServer &#x3D; optional.get().getServer();&#125; 坑二 使用了 filter 为什么没起作用呢？ 调用 filter 之后，它是有返回值的，所以你需要使用新的集合去接收 。。。 后续慢慢填坑","categories":[],"tags":[]}],"categories":[],"tags":[]}